{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82ac711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zetsu\\.conda\\envs\\master-thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270fe7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer and fix pad token\n",
    "model_path = \"./qwen-7b-lora-simplifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7dc962",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token or \"<|endoftext|>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37702598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model with quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f786b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:24<00:00,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen-7B-Chat\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6d9117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151851, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0e6f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): QWenLMHeadModel(\n",
       "      (transformer): QWenModel(\n",
       "        (wte): Embedding(151851, 4096)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (h): ModuleList(\n",
       "          (0-31): 32 x QWenBlock(\n",
       "            (ln_1): RMSNorm()\n",
       "            (attn): QWenAttention(\n",
       "              (c_attn): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ln_2): RMSNorm()\n",
       "            (mlp): QWenMLP(\n",
       "              (w1): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (w2): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=151851, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c59e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Batch of complex sentences to simplify\n",
    "complex_sentences = [\n",
    "#    \"The proliferation of autonomous vehicles necessitates robust regulatory oversight.\",\n",
    "#    \"Photosynthesis is a process through which green plants use sunlight to synthesize foods from carbon dioxide and water.\",\n",
    "#    \"Urban planners must incorporate resilient and adaptive infrastructure strategies to address climate-related risks.\",\n",
    "#    \"Quantum computing leverages principles of quantum mechanics to process information exponentially faster than classical computers.\"\n",
    "#    \"The amortization schedule delineates the temporal allocation of loan repayments, wherein a progressively increasing portion is attributed to the principal and a decreasing component to interest, under the assumption of a fixed interest rate and consistent periodic payments throughout the loan's tenure.\n",
    "    \"The teacher, the students, and the principal, who had all gathered in the auditorium for the emergency meeting, listened intently as the superintendent outlined the district's new safety protocols.\"\n",
    "]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2adf79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to simplify a sentence\n",
    "def simplify(text):\n",
    "    prompt = (\n",
    "    \"You are an assistant that rewrites technical sentences in plain English for students and non-experts. From this sentence, identify all the nouns, and then provide one overarching common noun that could represent the whole group. Use this common noun in the simplified sentence\\n\\n\"\n",
    "    f\"Sentence: {text.strip()}\\n\"\n",
    "    \"Simplified:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    simplified = output.split(\"Simplified:\")[-1].strip()\n",
    "    return simplified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a89126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch Simplification Results:\n",
      "\n",
      "1. Original:\n",
      "The teacher, the students, and the principal, who had all gathered in the auditorium for the emergency meeting, listened intently as the superintendent outlined the district's new safety protocols.\n",
      "   Simplified:\n",
      "People (which includes the teacher, students, principal, auditorium, emergency meeting participants, superintendent, and district) gathered in the auditorium to listen to the superintendent outline the district's new safety protocols.\n",
      "   ðŸ“Š Readability:\n",
      "     - Flesch Reading Ease: -17.19\n",
      "     - FK Grade Level: 22.90\n",
      "     - Gunning Fog: 19.88\n",
      "     - Consensus: 22nd and 23rd grade\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop through and simplify each sentence\n",
    "print(\" Batch Simplification Results:\\n\")\n",
    "for i, sentence in enumerate(complex_sentences, 1):\n",
    "    simplified = simplify(sentence)\n",
    "    print(f\"{i}. Original:\\n{sentence}\")\n",
    "    print(f\"   Simplified:\\n{simplified}\")\n",
    "    \n",
    "    # Optional: Readability metrics\n",
    "    print(\"   ðŸ“Š Readability:\")\n",
    "    print(f\"     - Flesch Reading Ease: {textstat.flesch_reading_ease(simplified):.2f}\")\n",
    "    print(f\"     - FK Grade Level: {textstat.flesch_kincaid_grade(simplified):.2f}\")\n",
    "    print(f\"     - Gunning Fog: {textstat.gunning_fog(simplified):.2f}\")\n",
    "    print(f\"     - Consensus: {textstat.text_standard(simplified)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb92431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
